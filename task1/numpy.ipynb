{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tsv = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "x_train = train_tsv['Phrase']\n",
    "y_train = np.array(train_tsv['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "class Vectorizer(object):\n",
    "    def __init__(self, ngram_range=(1,1), max_tf=1.0, use_tfidf=False, max_features=50000):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_tf = max_tf\n",
    "        self.use_tfidf = use_tfidf\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        self.vocabulary_ = dict()\n",
    "        self.counter_ = Counter()\n",
    "        self.df_ = []\n",
    "        self.idf_ = []\n",
    "        \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return list(filter(None, re.split('[\\W]', text.lower())))\n",
    "    \n",
    "    def get_ngrams(self, tokens):\n",
    "        ngrams = []\n",
    "        minlen = self.ngram_range[0]\n",
    "        maxlen = self.ngram_range[1]\n",
    "        for length in range(minlen, maxlen + 1):\n",
    "            for i in range(0, len(tokens) - length + 1):\n",
    "                ngrams.append(' '.join(tokens[i:i+length]))\n",
    "        return ngrams\n",
    "    \n",
    "    def fit(self, raw_documents):\n",
    "        self.counter_.clear()\n",
    "        counter_list = []\n",
    "\n",
    "        for doc in raw_documents:\n",
    "            tokens = self.tokenize(doc)\n",
    "            ngrams = self.get_ngrams(tokens)\n",
    "            counter = Counter(ngrams)\n",
    "            self.counter_.update(counter)\n",
    "            counter_list.append(counter)\n",
    "            \n",
    "        kv = sorted(self.counter_.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)[:self.max_features]\n",
    "        self.counter_ = Counter(dict(sorted(kv, key = lambda kv:(kv[0], kv[1]))))\n",
    "        self.vocabulary_ = dict([(k, i) for i, k in enumerate(self.counter_.keys())])\n",
    "        self.df_ = [0] * len(self.vocabulary_)\n",
    "        for counter in counter_list:\n",
    "            for key in counter.keys():\n",
    "                if key in self.vocabulary_:\n",
    "                    self.df_[self.vocabulary_[key]] += 1\n",
    "        self.df_ = np.array(self.df_)\n",
    "        self.idf_ = np.log((1.0 + len(counter_list))/ (1.0 + self.df_)) + 1.0\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        \n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        \n",
    "        for doc in raw_documents:\n",
    "            tokens = self.tokenize(doc)\n",
    "            ngrams = self.get_ngrams(tokens)\n",
    "            counter = Counter(ngrams)\n",
    "            \n",
    "            val = []\n",
    "            \n",
    "            for k, v in counter.items():\n",
    "                if k in self.vocabulary_:\n",
    "                    idx = self.vocabulary_[k]\n",
    "                    indices.append(idx)\n",
    "                \n",
    "                    if self.use_tfidf:\n",
    "                        val.append(v / sum(counter.values()) * self.idf_[idx])\n",
    "                    else:\n",
    "                        val.append(v)\n",
    "            \n",
    "            if self.use_tfidf:\n",
    "                val = np.array(val)\n",
    "                val /= np.sqrt((val**2).sum())\n",
    "                val = val.tolist()\n",
    "                \n",
    "            data += val\n",
    "            \n",
    "            indptr.append(len(data))\n",
    "            \n",
    "        return csr_matrix((data, indices, indptr), shape=(len(raw_documents), len(self.vocabulary_)))\n",
    "    \n",
    "    def fit_transform(self, raw_documents):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Vectorizer(ngram_range=(1,1), use_tfidf=False)\n",
    "train_counts = counter.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Vectorizer(ngram_range=(2,3), use_tfidf=True, max_features=50000)\n",
    "train_features = tfidf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 65275)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "train_ft = hstack([train_counts, train_features]).tocsr()\n",
    "print(train_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tsv = pd.read_csv('./data/test.tsv', sep='\\t')\n",
    "x_test = test_tsv['Phrase']\n",
    "test_counts = counter.transform(x_test)\n",
    "test_features = tfidf.transform(x_test)\n",
    "test_ft = hstack([test_counts, test_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_size=0.1, shuffle=True):\n",
    "    num = x.shape[0]\n",
    "    idx = np.arange(0, num)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    num_test = int(test_size * num)\n",
    "    num_train = num - num_test\n",
    "    return x[idx[:num_train]], x[idx[num_train:]],\\\n",
    "            y[idx[:num_train]], y[idx[num_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft, val_ft, train_label, val_label = train_test_split(train_ft, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=32, shuffle=True):\n",
    "    is_list = type(data) is list\n",
    "    if is_list:\n",
    "        num = data[0].shape[0] if type(data[0]) is csr_matrix else len(data[0])\n",
    "    else:\n",
    "        num = data.shape[0] if type(data) is csr_matrix else len(data)\n",
    "    idx = np.arange(num)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for left in np.arange(0, num, batch_size):\n",
    "        batch_idx = idx[left:left+batch_size]\n",
    "        yield [d[batch_idx] for d in data] if is_list else data[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_onehot(x, num_classes):\n",
    "    return np.eye(num_classes)[x]\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(-1, keepdims=True)\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, lr=1e-3, weight_decay=0.0):\n",
    "        self.w = np.random.uniform(size=(num_features, num_classes))\n",
    "        self.num_features = num_features\n",
    "        self.num_classes  = num_classes\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        probs = softmax(x.dot(self.w))\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = self.predict_prob(x)\n",
    "        return probs.argmax(-1)\n",
    "    \n",
    "    def gradient_descent(self, x, y):\n",
    "        probs = self.predict_prob(x)\n",
    "        gradients = x.transpose().dot(probs - to_onehot(y, self.num_classes))\n",
    "        self.w -= self.lr * (gradients + self.weight_decay * self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxRegression(train_ft.shape[1], 5, lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Accuracy = 39.5040369088812%\n",
      "Epoch 20: Accuracy = 43.60502370882994%\n",
      "Epoch 30: Accuracy = 46.46930667691913%\n",
      "Epoch 40: Accuracy = 47.99436114315007%\n",
      "Epoch 50: Accuracy = 49.45533769063181%\n",
      "Epoch 60: Accuracy = 50.46136101499423%\n",
      "Epoch 70: Accuracy = 51.31359733435858%\n",
      "Epoch 80: Accuracy = 51.94796872997565%\n",
      "Epoch 90: Accuracy = 52.5438933743432%\n",
      "Epoch 100: Accuracy = 53.08214789183647%\n",
      "Epoch 110: Accuracy = 53.37690631808279%\n",
      "Epoch 120: Accuracy = 53.71011149557862%\n",
      "Epoch 130: Accuracy = 54.395745226195054%\n",
      "Epoch 140: Accuracy = 54.555940023068054%\n",
      "Epoch 150: Accuracy = 54.64564910931693%\n",
      "Epoch 160: Accuracy = 54.97885428681276%\n",
      "Epoch 170: Accuracy = 55.164680251185445%\n",
      "Epoch 180: Accuracy = 55.4081763424324%\n",
      "Epoch 190: Accuracy = 55.5683711393054%\n",
      "Epoch 200: Accuracy = 55.65167243367935%\n",
      "Epoch 210: Accuracy = 55.74778931180315%\n",
      "Epoch 220: Accuracy = 55.86953735742663%\n",
      "Epoch 230: Accuracy = 56.10662565679867%\n",
      "Epoch 240: Accuracy = 55.99769319492503%\n",
      "Epoch 250: Accuracy = 56.145072408048186%\n",
      "Epoch 260: Accuracy = 56.22837370242215%\n",
      "Epoch 270: Accuracy = 56.15788799179803%\n",
      "Epoch 280: Accuracy = 56.31167499679611%\n",
      "Epoch 290: Accuracy = 56.45905420991926%\n",
      "Epoch 300: Accuracy = 56.39497629117006%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    for x, y in get_minibatch([train_ft, train_label], batch_size=256):\n",
    "        model.gradient_descent(x,y)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        num_correct = 0\n",
    "        for x, y in get_minibatch([val_ft, val_label], batch_size=256, shuffle=False):\n",
    "            num_correct += (model.predict(x) == y).sum()\n",
    "        print('Epoch {}: Accuracy = {}%'.format(epoch + 1, 100.0 * num_correct / len(val_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for x in get_minibatch(test_ft, batch_size=256, shuffle=False):\n",
    "    y_test += model.predict(x).tolist()\n",
    "    \n",
    "test_tsv['Sentiment'] = y_test\n",
    "test_tsv[['PhraseId', 'Sentiment']].to_csv('numpy_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr=3e-3, weight_decay=1e-3: 0.49380\n",
    "\n",
    "lr=3e-3, weight_decay=1e-2: 0.50517\n",
    "\n",
    "lr=1e-3, weight_decay=1e-2: 0.50526"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
